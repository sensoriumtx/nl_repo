{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: habanero in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: httpx>=0.27.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (0.28.1)\n",
      "Requirement already satisfied: packaging>=24.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (24.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.66.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (4.67.1)\n",
      "Collecting urllib3==2.2.0 (from habanero)\n",
      "  Using cached urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: anyio in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx>=0.27.2->habanero) (4.8.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx>=0.27.2->habanero) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx>=0.27.2->habanero) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx>=0.27.2->habanero) (3.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ubuntu/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.2->habanero) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.2->habanero) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.2->habanero) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.2->habanero) (4.12.2)\n",
      "Using cached urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.37.10 requires urllib3!=2.2.0,<3,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.0 which is incompatible.\n",
      "crossrefapi 1.6.0 requires urllib3==1.26.16, but you have urllib3 2.2.0 which is incompatible.\n",
      "evidently 0.4.40 requires cryptography>=43.0.1, but you have cryptography 41.0.7 which is incompatible.\n",
      "evidently 0.4.40 requires requests>=2.32.0, but you have requests 2.31.0 which is incompatible.\n",
      "pycaret 3.3.2 requires pandas<2.2.0, but you have pandas 2.2.1 which is incompatible.\n",
      "selenium 4.2.0 requires urllib3[secure,socks]~=1.26, but you have urllib3 2.2.0 which is incompatible.\n",
      "tensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install habanero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Neuroprotective potential of Ayahuasca and untargeted metabolomics analyses: applicability to Parkinson's disease\"]\n",
      "2020\n",
      "Katchborian-Neto\n"
     ]
    }
   ],
   "source": [
    "from habanero import Crossref\n",
    "\n",
    "cr = Crossref()\n",
    "\n",
    "# Lookup metadata from DOI\n",
    "result = cr.works(ids=\"10.1016/j.jep.2020.112743\")\n",
    "\n",
    "print(result['message']['title'])\n",
    "print(result['message']['published-print']['date-parts'][0][0])  # Year\n",
    "print(result['message']['author'][0]['family'])  # First author last name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: crossrefapi in /home/ubuntu/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: pyvis in /home/ubuntu/.local/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: PyMuPDF in /home/ubuntu/.local/lib/python3.10/site-packages (1.23.7)\n",
      "Requirement already satisfied: scispacy in /home/ubuntu/.local/lib/python3.10/site-packages (0.5.4)\n",
      "Requirement already satisfied: spacy in /home/ubuntu/.local/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (3.4.2)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.10/site-packages (3.7.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from crossrefapi) (2.31.0)\n",
      "Requirement already satisfied: urllib3==1.26.16 in /home/ubuntu/.local/lib/python3.10/site-packages (from crossrefapi) (1.26.16)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyvis) (8.10.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyvis) (3.1.6)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyvis) (4.0.2)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from PyMuPDF) (1.23.7)\n",
      "Requirement already satisfied: scipy<1.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from scispacy) (1.10.1)\n",
      "Requirement already satisfied: conllu in /home/ubuntu/.local/lib/python3.10/site-packages (from scispacy) (6.0.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from scispacy) (1.26.4)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.10/site-packages (from scispacy) (1.3.2)\n",
      "Requirement already satisfied: nmslib>=1.7.3.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from scispacy) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from scispacy) (1.4.2)\n",
      "Requirement already satisfied: pysbd in /home/ubuntu/.local/lib/python3.10/site-packages (from scispacy) (0.3.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (76.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: backcall in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /home/ubuntu/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2>=2.9.6->pyvis) (2.1.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: pybind11<2.6.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (2025.1.31)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (3.5.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/ubuntu/.local/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install crossrefapi pyvis PyMuPDF scispacy spacy networkx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'crossrefapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mscholarly\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m scholarly\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpyvis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnetwork\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Network\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcrossrefapi\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Crossref\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mPyPDF2\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m PdfReader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'crossrefapi'"
     ]
    }
   ],
   "source": [
    "# lineage_lit_spider.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scholarly import scholarly\n",
    "from pyvis.network import Network\n",
    "from crossrefapi import Crossref\n",
    "from PyPDF2 import PdfReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for DOI 10.3389/fphar.2023.1240295: EOF marker not found\n",
      "Failed for DOI 10.1007/s41465-019-00151-6: EOF marker not found\n",
      "Failed for DOI 10.1016/j.cell.2021.03.022: EOF marker not found\n",
      "Failed for DOI 10.1016/S2468-1253(19)30333-4: EOF marker not found\n",
      "Failed for DOI 10.3389/fmed.2019.00334: EOF marker not found\n",
      "Failed for DOI 10.1111/bph.13714: EOF marker not found\n",
      "Failed for DOI 10.1080/03344355.2020.1732046: EOF marker not found\n",
      "Failed for DOI 10.1186/1746-4269-10-26: EOF marker not found\n",
      "Failed for DOI 10.1007/s00109-011-0752-4: EOF marker not found\n",
      "Failed for DOI 10.1080/02791072.2019.1593560: EOF marker not found\n",
      "Failed for DOI 10.1136/jnnp-2019-320912: EOF marker not found\n",
      "Failed for DOI 10.1038/srep30550: EOF marker not found\n",
      "Failed for DOI 10.1007/s00213-007-0963-0: EOF marker not found\n",
      "Failed for DOI 10.4103/0253-7176.183086: EOF marker not found\n",
      "Failed for DOI 10.1016/j.paid.2017.06.004: EOF marker not found\n",
      "Failed for DOI 10.5152/eurjrheum.2017.17025: EOF marker not found\n",
      "Failed for DOI 10.1038/tp.2016.71: EOF marker not found\n",
      "Failed for DOI 10.1101/2020.07.14.202325: EOF marker not found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m# Cell 9: Execute Example\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m start_doi \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m10.3389/fphar.2023.1240295\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Replace with your starting paper\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m df \u001b[39m=\u001b[39m run_lineage_spider(start_doi, keywords\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mkava\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpfeiffer\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mepilepsy\u001b[39;49m\u001b[39m'\u001b[39;49m], max_layers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m build_network(df)\n",
      "\u001b[1;32m/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mDOI\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m ref:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m             spider(ref[\u001b[39m'\u001b[39m\u001b[39mDOI\u001b[39m\u001b[39m'\u001b[39m], layer\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, doi)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m spider(start_doi, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(lineage)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mbase_folder\u001b[39m}\u001b[39;00m\u001b[39m/lineage_output.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39mfor\u001b[39;00m ref \u001b[39min\u001b[39;00m meta[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mDOI\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m ref:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         spider(ref[\u001b[39m'\u001b[39;49m\u001b[39mDOI\u001b[39;49m\u001b[39m'\u001b[39;49m], layer\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, doi)\n",
      "\u001b[1;32m/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m meta \u001b[39m=\u001b[39m get_metadata(doi)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     pdf_path \u001b[39m=\u001b[39m download_pdf(doi, layer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     pages \u001b[39m=\u001b[39m extract_text_from_pdf(pdf_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     hits \u001b[39m=\u001b[39m find_keyword_contexts(pages, keywords)\n",
      "\u001b[1;32m/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdownload_pdf\u001b[39m(doi, layer):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://doi.org/\u001b[39m\u001b[39m{\u001b[39;00mdoi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mbase_folder\u001b[39m}\u001b[39;00m\u001b[39m/layer_\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mdoi\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.pdf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(file_path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    794\u001b[0m     conn,\n\u001b[1;32m    795\u001b[0m     method,\n\u001b[1;32m    796\u001b[0m     url,\n\u001b[1;32m    797\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    798\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    799\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    800\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    801\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    802\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    803\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    804\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    805\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    808\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1101\u001b[0m \u001b[39m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[0;32m--> 616\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    617\u001b[0m     server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    618\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    199\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    200\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    201\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    202\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m LocationParseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m     62\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# and socket type values to enum constants.\u001b[39;00m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m    957\u001b[0m     addrlist\u001b[39m.\u001b[39mappend((_intenum_converter(af, AddressFamily),\n\u001b[1;32m    958\u001b[0m                      _intenum_converter(socktype, SocketKind),\n\u001b[1;32m    959\u001b[0m                      proto, canonname, sa))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lineage_spider.ipynb\n",
    "\n",
    "# Cell 1: Imports\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from habanero import Crossref\n",
    "from PyPDF2 import PdfReader\n",
    "from collections import defaultdict\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Cell 2: Initialization\n",
    "cr = Crossref(mailto=\"nick.laskowski@sensorium.bio\")\n",
    "\n",
    "base_folder = 'literature_lineage_results'\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "# Cell 3: Extract PDF Text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = [page.extract_text() for page in reader.pages]\n",
    "    return pages\n",
    "\n",
    "# Cell 4: Keyword Hit Extraction\n",
    "def find_keyword_contexts(pages, keywords):\n",
    "    hits = []\n",
    "    for page_num, text in enumerate(pages):\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in text.lower():\n",
    "                hits.append({'keyword': keyword, 'page': page_num+1, 'evidence': text})\n",
    "    return hits\n",
    "\n",
    "# Cell 5: CrossRef Metadata\n",
    "def get_metadata(doi):\n",
    "    result = cr.works(ids=doi)\n",
    "    if result:\n",
    "        return {\n",
    "            'title': result['message']['title'][0] if 'title' in result['message'] else 'N/A',\n",
    "            'year': result['message']['issued']['date-parts'][0][0] if 'issued' in result['message'] else 'N/A',\n",
    "            'references': result['message'].get('reference', [])\n",
    "        }\n",
    "    return {'title': 'N/A', 'year': 'N/A', 'references': []}\n",
    "\n",
    "# Cell 6: Download PDF\n",
    "def download_pdf(doi, layer):\n",
    "    url = f'https://doi.org/{doi}'\n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    file_path = f'{base_folder}/layer_{layer}/{doi.replace(\"/\", \"_\")}.pdf'\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    return file_path\n",
    "\n",
    "# Cell 7: Main Spidering Function\n",
    "def run_lineage_spider(start_doi, keywords, max_layers=2):\n",
    "    lineage = []\n",
    "    seen = set()\n",
    "\n",
    "    def spider(doi, layer, source_doi='N/A'):\n",
    "        if doi in seen or layer > max_layers:\n",
    "            return\n",
    "        seen.add(doi)\n",
    "\n",
    "        meta = get_metadata(doi)\n",
    "        try:\n",
    "            pdf_path = download_pdf(doi, layer)\n",
    "            pages = extract_text_from_pdf(pdf_path)\n",
    "            hits = find_keyword_contexts(pages, keywords)\n",
    "        except Exception as e:\n",
    "            print(f'Failed for DOI {doi}: {e}')\n",
    "            hits = []\n",
    "\n",
    "        for hit in hits:\n",
    "            lineage.append({\n",
    "                'Layer': layer,\n",
    "                'DOI': doi,\n",
    "                'Title': meta['title'],\n",
    "                'Year': meta['year'],\n",
    "                'Source_DOI': source_doi,\n",
    "                'Keyword': hit['keyword'],\n",
    "                'Page': hit['page'],\n",
    "                'Evidence': hit['evidence']\n",
    "            })\n",
    "\n",
    "        for ref in meta['references']:\n",
    "            if 'DOI' in ref:\n",
    "                spider(ref['DOI'], layer+1, doi)\n",
    "\n",
    "    spider(start_doi, 1)\n",
    "\n",
    "    df = pd.DataFrame(lineage)\n",
    "    df.to_csv(f'{base_folder}/lineage_output.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Cell 8: Visualization\n",
    "def build_network(df):\n",
    "    net = Network(height='800px', width='100%')\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        net.add_node(row['DOI'], label=row['Title'][:50], title=row['Title'], shape='box')\n",
    "        if row['Source_DOI'] != 'N/A':\n",
    "            net.add_edge(row['Source_DOI'], row['DOI'])\n",
    "        entity_id = f\"{row['Keyword']}-{idx}\"\n",
    "        net.add_node(entity_id, label=row['Keyword'], shape='ellipse', color='orange')\n",
    "        net.add_edge(row['DOI'], entity_id, title=row['Evidence'][:200])\n",
    "\n",
    "    net.show(f'{base_folder}/lineage_network.html')\n",
    "\n",
    "# Cell 9: Execute Example\n",
    "start_doi = '10.3389/fphar.2023.1240295'  # Replace with your starting paper\n",
    "df = run_lineage_spider(start_doi, keywords=['kava', 'pfeiffer', 'epilepsy'], max_layers=2)\n",
    "build_network(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required libraries (only once per environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: habanero in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: PyMuPDF in /home/ubuntu/.local/lib/python3.10/site-packages (1.23.7)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: httpx>=0.27.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (0.28.1)\n",
      "Requirement already satisfied: packaging>=24.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (24.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (6.0.2)\n",
      "Requirement already satisfied: urllib3==2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from habanero) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from PyMuPDF) (1.23.7)\n",
      "Requirement already satisfied: anyio in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx>=0.27.2->habanero) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx>=0.27.2->habanero) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ubuntu/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.2->habanero) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.2->habanero) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.2->habanero) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.2->habanero) (4.12.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install habanero requests pandas PyMuPDF tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from habanero import Crossref\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set your email for Unpaywall API (required)\n",
    "UNPAYWALL_EMAIL = \"info@sensorium.bio\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Extract Metadata and References from a DOI using habanero\n",
    "\n",
    "This will help us:\n",
    "\n",
    "Retrieve the title, publication year, and references from a source paper.\n",
    "\n",
    "Prepare for citation traversal in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Crossref client\n",
    "cr = Crossref()\n",
    "\n",
    "def get_metadata_from_doi(doi):\n",
    "    \"\"\"\n",
    "    Get metadata including title, year, and references from a DOI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        record = cr.works(ids=doi)\n",
    "        title = record['message'].get('title', [''])[0]\n",
    "        year = record['message']['issued']['date-parts'][0][0]\n",
    "        references = record['message'].get('reference', [])\n",
    "        \n",
    "        # Extract DOIs from references if available\n",
    "        cited_dois = []\n",
    "        for ref in references:\n",
    "            if 'DOI' in ref:\n",
    "                cited_dois.append(ref['DOI'])\n",
    "\n",
    "        return {\n",
    "            'doi': doi,\n",
    "            'title': title,\n",
    "            'year': year,\n",
    "            'cited_dois': cited_dois\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving metadata for DOI {doi}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Kawa-Pyrone — eine neuartige Substanzgruppe zentraler Muskelrelaxantien vom Typ des Mephenesins\n",
      "Year: 1966\n",
      "Cited DOIs: ['10.1111/j.1749-6632.1960.tb42792.x', '10.1111/j.1749-6632.1956.tb36842.x', '10.1007/BF00420104']\n"
     ]
    }
   ],
   "source": [
    "# Example DOI (replace with your own)\n",
    "test_doi = \"10.1007/BF01711971\"\n",
    "\n",
    "meta = get_metadata_from_doi(test_doi)\n",
    "print(\"Title:\", meta['title'])\n",
    "print(\"Year:\", meta['year'])\n",
    "print(\"Cited DOIs:\", meta['cited_dois'][:5])  # Show first 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Query Unpaywall API to Retrieve Open Access PDF Links\n",
    "\n",
    "This step will:\n",
    "\n",
    "Query the Unpaywall API for the best open-access link.\n",
    "\n",
    "Attempt to download the PDF (if available).\n",
    "\n",
    "Save it to your directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting unpywall\n",
      "  Downloading unpywall-0.2.3.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (from unpywall) (2.2.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from unpywall) (2.31.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->unpywall) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->unpywall) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->unpywall) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->unpywall) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->unpywall) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->unpywall) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->unpywall) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->unpywall) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->unpywall) (1.16.0)\n",
      "Building wheels for collected packages: unpywall\n",
      "  Building wheel for unpywall (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unpywall: filename=unpywall-0.2.3-py3-none-any.whl size=12318 sha256=11add7f9235f92787e588d208fecebe77d1bcea2f638f85f7410c14eb92b7713\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/90/6b/7f/3a1299f56cc32bdd6d386c4fbf8d1ef8c7fcab39606ed63395\n",
      "Successfully built unpywall\n",
      "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: unpywall\n",
      "Successfully installed unpywall-0.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install unpywall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Unpywall' object has no attribute 'lookup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m unpaywall \u001b[39m=\u001b[39m Unpywall()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Correct method for Unpaywall API lookup\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/knowledge-graph/Nick_dev/git/nick_repo/20250326_literature_spider/lineage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m result \u001b[39m=\u001b[39m unpaywall\u001b[39m.\u001b[39;49mlookup(doi)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Unpywall' object has no attribute 'lookup'"
     ]
    }
   ],
   "source": [
    "from unpywall import Unpywall\n",
    "\n",
    "# Set your email\n",
    "Unpywall.email = \"nick.laskowski@sensorium.bio\"\n",
    "unpaywall = Unpywall()\n",
    "\n",
    "# Correct method for Unpaywall API lookup\n",
    "result = unpaywall.lookup(doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from habanero import cn\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "def get_unpaywall_pdf_url(doi):\n",
    "    try:\n",
    "        result = cn.content_negotiation(ids=doi, format=\"bibentry\", url=\"https://api.unpaywall.org/v2/\" + doi,\n",
    "                                        headers={\"Accept\": \"application/json\"},\n",
    "                                        mailto=\"nick.laskowski@sensorium.bio\")\n",
    "        \n",
    "        if result and 'best_oa_location' in result and result['best_oa_location']:\n",
    "            return result['best_oa_location']['url_for_pdf']\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unpaywall lookup failed for {doi}: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_pdf(doi, save_dir=\"literature_spider_results/layer_1/\"):\n",
    "    url = get_unpaywall_pdf_url(doi)\n",
    "    if url:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = doi.replace(\"/\", \"_\") + \".pdf\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        \n",
    "        print(f\"Attempting to download: {url}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {doi} to {save_path}\")\n",
    "            return save_path\n",
    "        else:\n",
    "            print(f\"Failed to download {doi} (HTTP {response.status_code})\")\n",
    "    else:\n",
    "        print(f\"No PDF found for {doi}\")\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpaywall lookup failed for 10.3389/fphar.2023.1240295: httpx.get() got multiple values for keyword argument 'headers'\n",
      "No PDF found for 10.3389/fphar.2023.1240295\n"
     ]
    }
   ],
   "source": [
    "doi = \"10.3389/fphar.2023.1240295\"\n",
    "download_pdf(doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "UNPAYWALL_API = \"https://api.unpaywall.org/v2/\"\n",
    "EMAIL = \"nick.laskowski@sensorium.bio\"\n",
    "\n",
    "def get_unpaywall_metadata(doi):\n",
    "    url = f\"{UNPAYWALL_API}{doi}?email={EMAIL}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Failed Unpaywall lookup for {doi} (HTTP {response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during Unpaywall request for {doi}: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_best_pdf_url(metadata):\n",
    "    if not metadata:\n",
    "        return None\n",
    "    location = metadata.get(\"best_oa_location\", {})\n",
    "    return location.get(\"url_for_pdf\")\n",
    "\n",
    "def download_pdf_from_doi(doi, save_dir=\"literature_spider_results/layer_1/\"):\n",
    "    metadata = get_unpaywall_metadata(doi)\n",
    "    pdf_url = get_best_pdf_url(metadata)\n",
    "    \n",
    "    if pdf_url:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = doi.replace(\"/\", \"_\") + \".pdf\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(pdf_url, stream=True, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"✅ Downloaded: {filepath}\")\n",
    "                return filepath\n",
    "            else:\n",
    "                print(f\"❌ PDF download failed (HTTP {response.status_code})\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Exception downloading PDF: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ No open-access PDF URL found for {doi}\")\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded: literature_spider_results/layer_1/10.3389_fphar.2023.1240295.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'literature_spider_results/layer_1/10.3389_fphar.2023.1240295.pdf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_pdf_from_doi(\"10.3389/fphar.2023.1240295\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity and Evidence Extraction Step Ready.\n"
     ]
    }
   ],
   "source": [
    "# Step: Lightweight Entity & Evidence Extraction\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load lightweight SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example custom dictionary (expand as needed)\n",
    "CUSTOM_TERMS = [\"kava\", \"psilocybin\", \"ayahuasca\", \"TNF-alpha\", \"Parkinson\", \"epilepsy\"]\n",
    "\n",
    "# Compile regex for fast matching\n",
    "custom_pattern = re.compile(r'|'.join([re.escape(term) for term in CUSTOM_TERMS]), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_entities_and_evidence(pages, source_doi, layer):\n",
    "    extracted_data = []\n",
    "\n",
    "    for page_number, page_text in enumerate(pages, start=1):\n",
    "        doc = nlp(page_text)\n",
    "        \n",
    "        # Named Entities\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"ORG\", \"GPE\", \"PERSON\", \"NORP\", \"PRODUCT\", \"DISEASE\"]:\n",
    "                evidence = ent.sent.text\n",
    "                extracted_data.append({\n",
    "                    \"Layer\": layer,\n",
    "                    \"DOI\": source_doi,\n",
    "                    \"Entity\": ent.text,\n",
    "                    \"Evidence_Statement\": evidence,\n",
    "                    \"Page_Number\": page_number\n",
    "                })\n",
    "\n",
    "        # Custom Terms\n",
    "        for match in custom_pattern.finditer(page_text):\n",
    "            term = match.group(0)\n",
    "            start_idx = match.start()\n",
    "\n",
    "            # Get surrounding sentence as evidence\n",
    "            sentences = list(doc.sents)\n",
    "            for sent in sentences:\n",
    "                if sent.start_char <= start_idx <= sent.end_char:\n",
    "                    extracted_data.append({\n",
    "                        \"Layer\": layer,\n",
    "                        \"DOI\": source_doi,\n",
    "                        \"Entity\": term,\n",
    "                        \"Evidence_Statement\": sent.text,\n",
    "                        \"Page_Number\": page_number\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# pages = extract_text_from_pdf(\"literature_spider_results/layer_1/10.3389_fphar.2023.1240295.pdf\")\n",
    "# data = extract_entities_and_evidence(pages, source_doi=\"10.3389/fphar.2023.1240295\", layer=1)\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv(\"entities_and_evidence_layer1.csv\", index=False)\n",
    "\n",
    "print(\"Entity and Evidence Extraction Step Ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bedside to bench: the outlook for\\npsychedelic...</td>\n",
       "      <td>[Victor, Bedside]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acero1,2,3,4, Emily S.</td>\n",
       "      <td>[Emily]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cribas4,5, Kevin D.</td>\n",
       "      <td>[Kevin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Browne1,2,\\nOlivia Rivellini1,2,4, Justin C.</td>\n",
       "      <td>[Olivia, Justin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Burrell1,2,3, John C.</td>\n",
       "      <td>[John]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O ’Donnell1,2,4,\\nSuradip Das1,2and D.</td>\n",
       "      <td>[Suradip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kacy Cullen1,2,3*\\n1Center for Brain Injury an...</td>\n",
       "      <td>[Injury, Repair, Corporal, Kacy, Philadelphia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Crescenz Veterans Affairs Medical Center, Phil...</td>\n",
       "      <td>[Crescenz, Applied, Microbiology, Engineering,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Despite promising ef ﬁcacy observed in some cl...</td>\n",
       "      <td>[Despite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indeed, most studies to date have focused on a...</td>\n",
       "      <td>[Indeed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  Bedside to bench: the outlook for\\npsychedelic...   \n",
       "1                             Acero1,2,3,4, Emily S.   \n",
       "2                                Cribas4,5, Kevin D.   \n",
       "3       Browne1,2,\\nOlivia Rivellini1,2,4, Justin C.   \n",
       "4                              Burrell1,2,3, John C.   \n",
       "5             O ’Donnell1,2,4,\\nSuradip Das1,2and D.   \n",
       "6  Kacy Cullen1,2,3*\\n1Center for Brain Injury an...   \n",
       "7  Crescenz Veterans Affairs Medical Center, Phil...   \n",
       "8  Despite promising ef ﬁcacy observed in some cl...   \n",
       "9  Indeed, most studies to date have focused on a...   \n",
       "\n",
       "                                            entities  \n",
       "0                                  [Victor, Bedside]  \n",
       "1                                            [Emily]  \n",
       "2                                            [Kevin]  \n",
       "3                                   [Olivia, Justin]  \n",
       "4                                             [John]  \n",
       "5                                          [Suradip]  \n",
       "6  [Injury, Repair, Corporal, Kacy, Philadelphia,...  \n",
       "7  [Crescenz, Applied, Microbiology, Engineering,...  \n",
       "8                                          [Despite]  \n",
       "9                                           [Indeed]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Lightweight Entity and Evidence Extraction\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Load small spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define patterns (add more as needed)\n",
    "chemical_pattern = r'\\b[A-Z][a-z]{2,}\\b'\n",
    "disease_keywords = [\"cancer\", \"epilepsy\", \"parkinson\", \"schizophrenia\", \"alzheimer\", \"inflammation\"]\n",
    "protein_keywords = [\"receptor\", \"enzyme\", \"cytokine\", \"protein\", \"gene\"]\n",
    "\n",
    "def extract_entities_and_evidence(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "    evidence_results = []\n",
    "\n",
    "    for sent in re.split(r'(?<=[.!?]) +', full_text):\n",
    "        doc = nlp(sent)\n",
    "        entities = set()\n",
    "\n",
    "        # Regex entity capture\n",
    "        entities.update(re.findall(chemical_pattern, sent))\n",
    "\n",
    "        # Keyword matches\n",
    "        for keyword in disease_keywords + protein_keywords:\n",
    "            if keyword.lower() in sent.lower():\n",
    "                entities.add(keyword)\n",
    "\n",
    "        if entities:\n",
    "            evidence_results.append({\n",
    "                \"sentence\": sent,\n",
    "                \"entities\": list(entities)\n",
    "            })\n",
    "\n",
    "    return evidence_results\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = 'fphar-14-1240295.pdf'\n",
    "evidence_data = extract_entities_and_evidence(pdf_path)\n",
    "\n",
    "# Display top evidence statements\n",
    "import pandas as pd\n",
    "pd.DataFrame(evidence_data).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(evidence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
