{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "CLASSIFIER_URL = \"https://npclassifier.gnps2.org/classify\"\n",
    "MAX_WORKERS = 64\n",
    "\n",
    "# SMILES classifier (one input → one output tuple)\n",
    "def classify_smiles(smiles):\n",
    "    try:\n",
    "        if not smiles or pd.isna(smiles) or smiles.strip().lower() == \"nan\":\n",
    "            return \"\", \"\", \"\", smiles\n",
    "        r = requests.get(CLASSIFIER_URL, params={\"smiles\": smiles}, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        result = r.json()\n",
    "\n",
    "        pathway = ', '.join(result.get('pathway_results', []))\n",
    "        superclass = ', '.join(result.get('superclass_results', []))\n",
    "        class_ = ', '.join(result.get('class_results', []))\n",
    "\n",
    "        if not any([pathway, superclass, class_]):\n",
    "            print(f\"[WARN] Empty classification for: {smiles}\")\n",
    "\n",
    "        return pathway, superclass, class_, smiles\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed: {smiles} → {str(e)}\")\n",
    "        with open(\"failed_smiles.txt\", \"a\") as f:\n",
    "            f.write(smiles + \"\\n\")\n",
    "        return \"\", \"\", \"\", smiles\n",
    "\n",
    "# Process a chunk (strict row alignment)\n",
    "def process_chunk(chunk_df, chunk_index, outdir):\n",
    "    smiles_list = chunk_df['primary_SMILES'].fillna('').astype(str).tolist()\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(classify_smiles, s) for s in smiles_list]\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            results.append(future.result())\n",
    "            if (i + 1) % 50 == 0 or (i + 1) == len(futures):\n",
    "                print(f\"[Chunk {chunk_index}] Processed {i + 1}/{len(futures)}\")\n",
    "\n",
    "    # Reorder results to original input order using index\n",
    "    results_dict = {res[3]: res for res in results}\n",
    "    ordered = [results_dict.get(s, (\"\", \"\", \"\", s)) for s in smiles_list]\n",
    "\n",
    "    # Attach results\n",
    "    chunk_df['Pathway Results'] = [r[0] for r in ordered]\n",
    "    chunk_df['Superclass Results'] = [r[1] for r in ordered]\n",
    "    chunk_df['Class Results'] = [r[2] for r in ordered]\n",
    "\n",
    "    outfile = os.path.join(outdir, f\"output_chunk_{chunk_index}.csv\")\n",
    "    chunk_df.to_csv(outfile, index=False)\n",
    "    print(f\"[SAVED] Chunk {chunk_index} → {outfile}\")\n",
    "\n",
    "# Merge chunks\n",
    "def merge_chunks(outdir, final_outfile):\n",
    "    files = sorted(glob.glob(os.path.join(outdir, 'output_chunk_*.csv')))\n",
    "    all_chunks = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "    all_chunks.to_csv(final_outfile, index=False)\n",
    "    print(f\"[MERGED] Output → {final_outfile}\")\n",
    "\n",
    "# Pipeline entrypoint\n",
    "def run_pipeline(input_file, outdir=\"classified_chunks\", chunk_size=10000):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    df = pd.read_csv(input_file)[['cmp', 'primary_SMILES']].dropna()\n",
    "\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "    print(f\"[INFO] Classifying {len(df)} SMILES across {total_chunks} chunks...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(total_chunks):\n",
    "        print(f\"\\n[START] Chunk {i + 1}/{total_chunks}\")\n",
    "        chunk_df = df.iloc[i * chunk_size : (i + 1) * chunk_size].copy()\n",
    "        t0 = time.time()\n",
    "        process_chunk(chunk_df, i + 1, outdir)\n",
    "        print(f\"[DONE] Chunk {i + 1} in {time.time() - t0:.2f}s\")\n",
    "\n",
    "    final_csv = os.path.join(outdir, \"npclassifier_output.csv\")\n",
    "    merge_chunks(outdir, final_csv)\n",
    "    print(f\"[FINISHED] Total time: {time.time() - start_time:.2f}s\")\n",
    "    return final_csv\n",
    "\n",
    "\n",
    "input_path = \"2025-04-07_master_cmp_scoring_dev_w_priority.csv\"\n",
    "output_dir = \"pain_np_leftover\"\n",
    "final_output = run_pipeline(input_path, outdir=output_dir, chunk_size=10000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
