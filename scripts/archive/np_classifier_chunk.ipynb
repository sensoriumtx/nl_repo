{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "CLASSIFIER_URL = \"https://npclassifier.gnps2.org/classify\"\n",
    "\n",
    "# Max workers for API requests (increase gradually; test 64â€“100)\n",
    "MAX_WORKERS = 64\n",
    "\n",
    "def classify_smiles(smiles):\n",
    "    try:\n",
    "        if not smiles or pd.isna(smiles) or smiles.lower().strip() == 'nan':\n",
    "            return \"\", \"\", \"\", smiles\n",
    "        r = requests.get(CLASSIFIER_URL, params={\"smiles\": smiles}, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        result = r.json()\n",
    "        return (\n",
    "            ', '.join(result.get('pathway_results', [])),\n",
    "            ', '.join(result.get('superclass_results', [])),\n",
    "            ', '.join(result.get('class_results', [])),\n",
    "            smiles\n",
    "        )\n",
    "    except Exception:\n",
    "        with open(\"failed_smiles.txt\", \"a\") as f:\n",
    "            f.write(smiles + \"\\n\")\n",
    "        return \"\", \"\", \"\", smiles\n",
    "\n",
    "def process_chunk(chunk_df, chunk_index, outdir):\n",
    "    smiles_list = chunk_df['primary_SMILES'].fillna('').astype(str).tolist()\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_smiles = {executor.submit(classify_smiles, s): s for s in smiles_list}\n",
    "        for future in as_completed(future_to_smiles):\n",
    "            results.append(future.result())\n",
    "\n",
    "    pathways, superclasses, classes, smiles_out = zip(*results)\n",
    "    chunk_df['primary_SMILES'] = smiles_out\n",
    "    chunk_df['Pathway Results'] = pathways\n",
    "    chunk_df['Superclass Results'] = superclasses\n",
    "    chunk_df['Class Results'] = classes\n",
    "\n",
    "    outfile = os.path.join(outdir, f\"output_chunk_{chunk_index}.csv\")\n",
    "    chunk_df.to_csv(outfile, index=False)\n",
    "    print(f\"Saved chunk {chunk_index}: {outfile}\")\n",
    "\n",
    "def merge_chunks(outdir, final_outfile):\n",
    "    files = sorted(glob.glob(os.path.join(outdir, 'output_chunk_*.csv')))\n",
    "    all_chunks = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "    all_chunks.to_csv(final_outfile, index=False)\n",
    "    print(f\"Merged output written to: {final_outfile}\")\n",
    "\n",
    "def run_pipeline(input_file, outdir=\"classified_chunks\", chunk_size=10000):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    df = pd.read_csv(input_file)\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "    print(f\"Classifying {len(df)} SMILES in {total_chunks} chunks of {chunk_size} using {MAX_WORKERS} threads...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(total_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, len(df))\n",
    "        chunk_df = df.iloc[start:end].copy()\n",
    "        t0 = time.time()\n",
    "        process_chunk(chunk_df, i + 1, outdir)\n",
    "        print(f\"Chunk {i + 1} completed in {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    print(f\"\\nAll chunks done in {time.time() - start_time:.2f} seconds\")\n",
    "    merge_chunks(outdir, os.path.join(outdir, \"final_npclassifier_output.csv\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\n",
    "        input_file=\"2025-04-07_master_cmp_scoring_dev_w_priority.csv\",\n",
    "        outdir=\"classified_chunks_fast\",\n",
    "        chunk_size=10000\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
