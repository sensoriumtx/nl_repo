{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "CLASSIFIER_URL = \"https://npclassifier.gnps2.org/classify\"\n",
    "MAX_WORKERS = 64  # Tune based on your machine\n",
    "\n",
    "def classify_smiles(smiles):\n",
    "    try:\n",
    "        if not smiles or pd.isna(smiles) or smiles.lower().strip() == 'nan':\n",
    "            return \"\", \"\", \"\", smiles\n",
    "        r = requests.get(CLASSIFIER_URL, params={\"smiles\": smiles}, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        result = r.json()\n",
    "\n",
    "        pathway = ', '.join(result.get('pathway_results', []))\n",
    "        superclass = ', '.join(result.get('superclass_results', []))\n",
    "        class_ = ', '.join(result.get('class_results', []))\n",
    "\n",
    "        if not any([pathway, superclass, class_]):\n",
    "            print(f\"[WARN] Empty result for: {smiles}\")\n",
    "\n",
    "        return pathway, superclass, class_, smiles\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed SMILES: {smiles} | Reason: {str(e)}\")\n",
    "        with open(\"failed_smiles.txt\", \"a\") as f:\n",
    "            f.write(smiles + \"\\n\")\n",
    "        return \"\", \"\", \"\", smiles\n",
    "\n",
    "def resolve_smiles_column(df):\n",
    "    for col in ['isoSmiles', 'primary_SMILES', 'smiles']:\n",
    "        if col in df.columns:\n",
    "            df['isoSmiles'] = df[col].astype(str)\n",
    "            return df\n",
    "    raise ValueError(\"No valid SMILES column found. Expect one of: isoSmiles, primary_SMILES, smiles\")\n",
    "\n",
    "def process_chunk(chunk_df, chunk_index, outdir):\n",
    "    smiles_list = chunk_df['isoSmiles'].fillna('').astype(str).tolist()\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_smiles = {executor.submit(classify_smiles, s): s for s in smiles_list}\n",
    "        for future in as_completed(future_to_smiles):\n",
    "            results.append(future.result())\n",
    "\n",
    "    # Sort results back to original input order\n",
    "    results_sorted = [r for _, r in sorted(zip(future_to_smiles.keys(), results), key=lambda x: smiles_list.index(x[1][3]))]\n",
    "\n",
    "    # Unpack results\n",
    "    pathways, superclasses, classes, smiles_out = zip(*results_sorted)\n",
    "    chunk_df['isoSmiles'] = smiles_out\n",
    "    chunk_df['Pathway Results'] = pathways\n",
    "    chunk_df['Superclass Results'] = superclasses\n",
    "    chunk_df['Class Results'] = classes\n",
    "\n",
    "    outfile = os.path.join(outdir, f\"output_chunk_{chunk_index}.csv\")\n",
    "    chunk_df.to_csv(outfile, index=False)\n",
    "    print(f\"[SAVED] Chunk {chunk_index}: {outfile}\")\n",
    "def merge_chunks(outdir, final_outfile):\n",
    "    files = sorted(glob.glob(os.path.join(outdir, 'output_chunk_*.csv')))\n",
    "    all_chunks = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "    all_chunks.to_csv(final_outfile, index=False)\n",
    "    print(f\"[MERGED] Output written to: {final_outfile}\")\n",
    "\n",
    "def run_pipeline(input_file, outdir=\"classified_chunks\", chunk_size=10000):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    df = pd.read_csv(input_file)\n",
    "    df = resolve_smiles_column(df)\n",
    "\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "    print(f\"[INFO] Classifying {len(df)} SMILES in {total_chunks} chunks of {chunk_size}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(total_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, len(df))\n",
    "        chunk_df = df.iloc[start:end].copy()\n",
    "        t0 = time.time()\n",
    "        process_chunk(chunk_df, i + 1, outdir)\n",
    "        print(f\"[DONE] Chunk {i + 1} in {time.time() - t0:.2f}s\")\n",
    "\n",
    "    merge_chunks(outdir, os.path.join(outdir, \"final_npclassifier_output.csv\"))\n",
    "    print(f\"[DONE] Full run in {time.time() - start_time:.2f}s\")\n",
    "# Create a test CSV in notebook\n",
    "# test_smiles_df = pd.DataFrame({\"isoSmiles\": [\"COc1ccc(/C=C/c2cc(OC)cc(=O)o2)cc1\"]})\n",
    "test_input_path = \"2025-04-07_master_cmp_scoring_dev_w_priority.csv\"\n",
    "test_output_dir = \"2025-06-03_master_cmp_with_np.csv\"\n",
    "#test_smiles_df.to_csv(test_input_path, index=False)\n",
    "\n",
    "# Run\n",
    "run_pipeline(test_input_path, outdir=test_output_dir, chunk_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
